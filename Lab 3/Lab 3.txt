Assignment 1
Write mlParams(X, labels)
Plot ML-estimates with 95 % confidence interval

Assignment 2
Write function computePrior(labels)
Write function classifyBayes(X,prior,mu,sigma)

Assignment 3
(1) When can a feature independence assumption be reasonable and when not?

(2) How does the decision boundary look for the Iris dataset ?
See image.

(3) How would one improve the classification results for this scenario by changing 
classifier or, alternatively, manipulate the Iris dataset?
To improve, I would continue through this lab and hope the results gets better :) 
otherwise I would maybe us a svm with some slack. Because it looks kinda like a problem
where that model would be useful. Some perceptron-model should also be able to solve this
problem quite well.

Assignment 4
Update mlParams


Assignment 5

Iris w/o boosting
Trial: 0 Accuracy 84.4
Trial: 10 Accuracy 95.6
Trial: 20 Accuracy 93.3
Trial: 30 Accuracy 86.7
Trial: 40 Accuracy 88.9
Trial: 50 Accuracy 91.1
Trial: 60 Accuracy 86.7
Trial: 70 Accuracy 91.1
Trial: 80 Accuracy 86.7
Trial: 90 Accuracy 91.1
Final mean classification accuracy  89 with standard deviation 4.16
vowel w/o boosting
Trial: 0 Accuracy 61
Trial: 10 Accuracy 66.2
Trial: 20 Accuracy 74
Trial: 30 Accuracy 66.9
Trial: 40 Accuracy 59.7
Trial: 50 Accuracy 64.3
Trial: 60 Accuracy 66.9
Trial: 70 Accuracy 63.6
Trial: 80 Accuracy 62.3
Trial: 90 Accuracy 70.8
Final mean classification accuracy  64.7 with standard deviation 4.03

iris w boosting
Trial: 0 Accuracy 95.6
Trial: 10 Accuracy 97.8
Trial: 20 Accuracy 93.3
Trial: 30 Accuracy 93.3
Trial: 40 Accuracy 97.8
Trial: 50 Accuracy 93.3
Trial: 60 Accuracy 95.6
Trial: 70 Accuracy 95.6
Trial: 80 Accuracy 93.3
Trial: 90 Accuracy 93.3
Final mean classification accuracy  94.5 with standard deviation 2.92
vowel w boosting
Trial: 0 Accuracy 68.2
Trial: 10 Accuracy 68.8
Trial: 20 Accuracy 74
Trial: 30 Accuracy 70.1
Trial: 40 Accuracy 67.5
Trial: 50 Accuracy 69.5
Trial: 60 Accuracy 72.1
Trial: 70 Accuracy 68.8
Trial: 80 Accuracy 73.4
Trial: 90 Accuracy 70.8
Final mean classification accuracy  71.3 with standard deviation 4.62

(1) Is there any improvement in classication accuracy? Why/why not?
Yes, the results are better with 5-6 points. On the iris set that almost removes half the
errors existing before, and it is a good improvement on the vowel set as well. This is 
probably because the datasets are to complex to classify with only the naives bayes, this 
more complex solution provides a good complement to the existing. If we only look at the line 
in between class 2 and 3 in the iris data set, the weights in the boost probably makes the
classifiers that want to draw the line with a slight curve to the left less important. It
also helps that the classifiers train on multiple variants of the datasets.

(2) Plot the decision boundary of the boosted classifier on iris and compare it with
that of the basic. What diferences do you notice? Is the boundary of the boosted
version more complex?
I would not say more complex, how ever it seems to fit the dataset better.

(3) Can we make up for not using a more advanced model in the basic classifier
(e.g. independent features) by using boosting? 
Well yes, the results are improved. So, yes at least in the scenarios given in this lab.

Assignment 6

